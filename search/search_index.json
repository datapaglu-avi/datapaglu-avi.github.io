{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hey Everyone \ud83d\udc4b\ud83c\udffb I am Avi Kasliwal - The \"Data Paglu\". I chase data for living - and for fun. With over 4.5 years of experience in the data world. I've worked as a data consultant at ZS Associates and currently serving as BI Engineer at Blinkit. My Expertise lies in data warehouing, ETL pipelines, dbt modeling, and automation with Python. Why \"Data Paglu\"? Because data isn't just my job - it's my playground. Wether I am analyzing my Dream 11 teams, comapring stocks, or hunting for the best price for wishlisted item at Amazon, I love using data to make smart, real-world decisions. What's here This website is where I share my side projects, my learnings, my experiences where I benefit or loss cause of data analysis, and connect with fellow data enthusiasts and potential collaborators. You can reach out to me here Linkedin","title":"Hey Everyone \ud83d\udc4b\ud83c\udffb"},{"location":"#hey-everyone","text":"I am Avi Kasliwal - The \"Data Paglu\". I chase data for living - and for fun. With over 4.5 years of experience in the data world. I've worked as a data consultant at ZS Associates and currently serving as BI Engineer at Blinkit. My Expertise lies in data warehouing, ETL pipelines, dbt modeling, and automation with Python.","title":"Hey Everyone \ud83d\udc4b\ud83c\udffb"},{"location":"#why-data-paglu","text":"Because data isn't just my job - it's my playground. Wether I am analyzing my Dream 11 teams, comapring stocks, or hunting for the best price for wishlisted item at Amazon, I love using data to make smart, real-world decisions.","title":"Why \"Data Paglu\"?"},{"location":"#whats-here","text":"This website is where I share my side projects, my learnings, my experiences where I benefit or loss cause of data analysis, and connect with fellow data enthusiasts and potential collaborators.","title":"What's here"},{"location":"#you-can-reach-out-to-me-here","text":"Linkedin","title":"You can reach out to me here"},{"location":"projects/tl%3Bdl/","text":"TL;DL \u2014 Your Commute-Sized Daily Dose of YouTube TL;DL (Too Long; Didn\u2019t Listen) is my weekend side project that came out of pure necessity. \ud83d\ude8c The Problem: YouTube \u2260 Commute-Friendly I spend about 30-45 minutes commuting between home and office \u2014 a perfect window to catch up on the latest in business, geopolitics, sports, and tech. Naturally, I built a habit of queuing up YouTube videos, speeding them up to 1.5x, and diving in. But there was a problem: Total length of videos go well over an hour. I couldn\u2019t always finish them during the commute. Some days, I had to watch them late at night or just skip them entirely. Over time, my watch queue became an unread inbox \ud83d\ude29 \ud83c\udfa7 The Lightbulb Moment: Podcasts + AI Then I came across NotebookLM by Google and its audio summary feature \u2014 that\u2019s when it hit me: Why not convert YouTube videos into concise podcast-style summaries? Audio is perfect for my commute. AI can compress the fluff. Bonus: I don\u2019t have to watch anything. Thus, TL;DL was born. \ud83d\udee0\ufe0f The Stack: Simple, Smart, and Efficient I didn\u2019t want to reinvent the wheel or burn API tokens unnecessarily. So I stitched together a neat little pipeline with open-source tools and free APIs. \ud83d\udd04 1. Channel Tracking with RSS Discovered YouTube offers an RSS feed for every channel. No scraping. No quota limits. Just clean metadata (title, video ID, publish date). Filtered videos from the last 24 hours for freshness. Example for RSS Feed \ud83d\udcdc 2. Transcript Extraction Used a Python library ( youtube_transcript_api ) to pull subtitles programmatically. No need to download the video or fiddle with any third-party tools. \ud83e\udd16 3. Summarization with ChatGPT Sent the transcript to OpenAI\u2019s API via LangChain. Translated (if needed) and summarized into crisp, conversational scripts. \ud83d\udde3\ufe0f 4. Text to Speech Found a free TTS engine using Microsoft Edge voices ( edge-tts ). Not Google-quality, but good enough for now. Planning to switch to Google Gemini 2 voices next. \u2b06\ufe0f 5. Podcast Publishing Used the YouTube Data API v3 to upload the audio as a video. Dockerized the whole thing and scheduled it to run every morning \ud83d\ude80 \ud83c\udfaf What\u2019s Next This is just v0.1 \u2014 very much a personal project right now. But here\u2019s where it\u2019s heading: \ud83d\udc65 Two-Speaker Format Make the audio feel more dynamic and natural, with alternating voices. Adds personality to the summaries. \u2699\ufe0f Configurable for Others Let users plug in their own channels, categories, or languages. Might open source it or wrap it into a simple web interface. \ud83e\uddfc Code Cleanup & Polish Refactor the Python + shell scripts for better readability and structure. Add: Fail checks & retries Logging pydantic for validation Docstrings and typed functions Move away from calling Python files directly from Docker shell \u2014 not proud of that bit \ud83d\ude48 \ud83d\udca1 Why This Matters We\u2019re drowning in content. TL;DL tries to fight back with: Curation (latest videos only) Compression (summarized) Conversion (text \u2192 audio \u2192 podcast) If you're like me \u2014 short on time but high on curiosity \u2014 I think you\u2019ll love what this turns into. Stay tuned. And maybe... stay subscribed? \ud83d\ude04","title":"TL;DL \u2014 Your Commute-Sized Daily Dose of YouTube"},{"location":"projects/tl%3Bdl/#tldl-your-commute-sized-daily-dose-of-youtube","text":"TL;DL (Too Long; Didn\u2019t Listen) is my weekend side project that came out of pure necessity.","title":"TL;DL \u2014 Your Commute-Sized Daily Dose of YouTube"},{"location":"projects/tl%3Bdl/#the-problem-youtube-commute-friendly","text":"I spend about 30-45 minutes commuting between home and office \u2014 a perfect window to catch up on the latest in business, geopolitics, sports, and tech. Naturally, I built a habit of queuing up YouTube videos, speeding them up to 1.5x, and diving in. But there was a problem: Total length of videos go well over an hour. I couldn\u2019t always finish them during the commute. Some days, I had to watch them late at night or just skip them entirely. Over time, my watch queue became an unread inbox \ud83d\ude29","title":"\ud83d\ude8c The Problem: YouTube \u2260 Commute-Friendly"},{"location":"projects/tl%3Bdl/#the-lightbulb-moment-podcasts-ai","text":"Then I came across NotebookLM by Google and its audio summary feature \u2014 that\u2019s when it hit me: Why not convert YouTube videos into concise podcast-style summaries? Audio is perfect for my commute. AI can compress the fluff. Bonus: I don\u2019t have to watch anything. Thus, TL;DL was born.","title":"\ud83c\udfa7 The Lightbulb Moment: Podcasts + AI"},{"location":"projects/tl%3Bdl/#the-stack-simple-smart-and-efficient","text":"I didn\u2019t want to reinvent the wheel or burn API tokens unnecessarily. So I stitched together a neat little pipeline with open-source tools and free APIs.","title":"\ud83d\udee0\ufe0f The Stack: Simple, Smart, and Efficient"},{"location":"projects/tl%3Bdl/#1-channel-tracking-with-rss","text":"Discovered YouTube offers an RSS feed for every channel. No scraping. No quota limits. Just clean metadata (title, video ID, publish date). Filtered videos from the last 24 hours for freshness. Example for RSS Feed","title":"\ud83d\udd04 1. Channel Tracking with RSS"},{"location":"projects/tl%3Bdl/#2-transcript-extraction","text":"Used a Python library ( youtube_transcript_api ) to pull subtitles programmatically. No need to download the video or fiddle with any third-party tools.","title":"\ud83d\udcdc 2. Transcript Extraction"},{"location":"projects/tl%3Bdl/#3-summarization-with-chatgpt","text":"Sent the transcript to OpenAI\u2019s API via LangChain. Translated (if needed) and summarized into crisp, conversational scripts.","title":"\ud83e\udd16 3. Summarization with ChatGPT"},{"location":"projects/tl%3Bdl/#4-text-to-speech","text":"Found a free TTS engine using Microsoft Edge voices ( edge-tts ). Not Google-quality, but good enough for now. Planning to switch to Google Gemini 2 voices next.","title":"\ud83d\udde3\ufe0f 4. Text to Speech"},{"location":"projects/tl%3Bdl/#5-podcast-publishing","text":"Used the YouTube Data API v3 to upload the audio as a video. Dockerized the whole thing and scheduled it to run every morning \ud83d\ude80","title":"\u2b06\ufe0f 5. Podcast Publishing"},{"location":"projects/tl%3Bdl/#whats-next","text":"This is just v0.1 \u2014 very much a personal project right now. But here\u2019s where it\u2019s heading:","title":"\ud83c\udfaf What\u2019s Next"},{"location":"projects/tl%3Bdl/#two-speaker-format","text":"Make the audio feel more dynamic and natural, with alternating voices. Adds personality to the summaries.","title":"\ud83d\udc65 Two-Speaker Format"},{"location":"projects/tl%3Bdl/#configurable-for-others","text":"Let users plug in their own channels, categories, or languages. Might open source it or wrap it into a simple web interface.","title":"\u2699\ufe0f Configurable for Others"},{"location":"projects/tl%3Bdl/#code-cleanup-polish","text":"Refactor the Python + shell scripts for better readability and structure. Add: Fail checks & retries Logging pydantic for validation Docstrings and typed functions Move away from calling Python files directly from Docker shell \u2014 not proud of that bit \ud83d\ude48","title":"\ud83e\uddfc Code Cleanup &amp; Polish"},{"location":"projects/tl%3Bdl/#why-this-matters","text":"We\u2019re drowning in content. TL;DL tries to fight back with: Curation (latest videos only) Compression (summarized) Conversion (text \u2192 audio \u2192 podcast) If you're like me \u2014 short on time but high on curiosity \u2014 I think you\u2019ll love what this turns into. Stay tuned. And maybe... stay subscribed? \ud83d\ude04","title":"\ud83d\udca1 Why This Matters"}]}